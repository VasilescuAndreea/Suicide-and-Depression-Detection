# -*- coding: utf-8 -*-
"""Copy of Suicide and Depression Detection V2 -> So I can test in parallel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/123XhbUhOHXhao4r30S5arsn9_zXTUh-I

> Suicide and depression detection in Reddut messages

Connect google
"""

from google.colab import drive
drive.mount('/content/gdrive')

! ls -s /content/gdrive/MyDrive/SuicideAndDepressionDetection

"""Imports

"""

import numpy as np
import pandas as pd
import os
import torch
import sklearn
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import _stop_words
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier

"""### **READ THE DATA**

---


"""

df = pd.read_csv("/content/gdrive/MyDrive/SuicideAndDepressionDetection/Suicide_Detection.csv", header = None, names = ["index", "text", "class"])
df.head()

len(df)

df["class"]

"""Less data than original because session crashed :/"""

input_size = int(len(df)/7)
input_size

reddit_paths = df.loc[1:input_size, "index"].tolist()
reddits = df.loc[1:input_size, "text"].tolist()
reddit_class = df.loc[1:input_size, "class"].tolist()

len(reddit_paths)

print(reddits[1])
reddit_class[1]

"""## PREPROCESSING


---

###Removed stop words

Corpus will contain reddits, but without stop words
"""

import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
stopwords = set(stopwords.words('english'))
corpus = []
for sentence in reddits:
  words = sentence.split(" ")
  filtered_words = [word.lower() for word in words if not word in stopwords]
  corpus.append(" ".join(filtered_words))
corpus

print(stopwords)

corpus[2]

reddits[2]



"""##### **CLEANING THE TEXT**



1.   Lowercase reddits
2.   Remove whitespace
3.   Remove numbers
4.   Remove special characters
5.   Remove emails
6.   Remove stop words
7.   Remove NAN
8.   Remove links
9.   Tokenize:
Tokenization refers to the process of breaking down a text or document into smaller units called tokens. Tokens can be individual words, sentences, or even smaller units like characters or subwords, depending on the specific tokenization scheme used.



"""

import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.corpus import stopwords
import re
import nltk
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

def cleanText(text):
  text = str(text)
  text = text.lower()
  text = text.replace('{html}', "")
  cleanr = re.compile('<.*?>')
  cleantext = re.sub(cleanr, '', text)
  rem_url = re.sub(r'http\S+', '', cleantext)
  rem_num = re.sub('[0-9]+', '', rem_url)
  tokenizer = RegexpTokenizer(r'\w+')
  tokens = tokenizer.tokenize(rem_num)
  filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]
  stem_words = [stemmer.stem(w) for w in filtered_words]
  lemma_words = [lemmatizer.lemmatize(w) for w in stem_words]
  return " ".join(filtered_words)

df['text'][1:input_size]

len(df['text'][0:input_size])

df['text'][1]

df['cleanText'] = df['text'][1:input_size].map(lambda s:cleanText(s))

cleanReddits = df['cleanText'][0:input_size]
len(cleanReddits)

cleanReddits = df.loc[1:input_size, "cleanText"].tolist()
len(cleanReddits)

input_size

cleanReddits[71]

reddits[71]

reddit_class[71]

"""### NER(Named Entity Recognition)

is a subtask of info extraction that aims to identify and classify named entities within text into predefined categories such as person names, location or organization.
"""

txt = cleanReddits

"""**Spacy information**

spacy.load("en_core_web_sm") is a Python code snippet that loads a specific language model in spaCy, which is a popular open-source library used for natural language processing (NLP) tasks.

In this case, "en_core_web_sm" refers to the name of the English language model provided by spaCy. The model is trained on a large corpus of English text and contains pre-trained word vectors, linguistic annotations, and various NLP features.

By executing spacy.load("en_core_web_sm"), you are loading the English language model into memory, allowing you to perform various NLP tasks such as tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more.
"""

import spacy
from spacy import displacy
spacy.load("en_core_web_sm")

"""Test just for one to see how it works"""

txt = cleanReddits[1]
tokenizer = spacy.load("en_core_web_sm")
adj=[]
token_list = tokenizer(txt)
for token in token_list:
    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))
    if (token.pos_ == 'ADJ'):
      adj.append(token)

"""ADJ in cleaned ds"""

tokenizer = spacy.load("en_core_web_sm")
adj = []
for redd in range((len(cleanReddits)-1)):
  txt = cleanReddits[redd]
  token_list = tokenizer(txt)
  for token in token_list:
    if (token.pos_ == 'ADJ'):
      adj.append(token)

adj

tokenizer = spacy.load("en_core_web_sm")
token_list = tokenizer(txt)
for token in token_list.ents:
    print(token.text, '->', token.label_)

import os
from collections import Counter
import seaborn as sns

adjectives = ' '.join(map(str, adj))

d = os.getcwd()
freq = Counter(adjectives.split(" "))
sns.set_style("darkgrid")
words = [word[0] for word in freq.most_common(10)]
count = [word[1] for word in freq.most_common(10)]

plt.figure(figsize=(10, 10))

sns_bar = sns.barplot(x=words, y=count)
sns_bar.set_xticklabels(words, rotation=90)
plt.title('Most Used Adjectives')
plt.show()

"""*   reddit_paths -> indexes for all reddits -> df['index']
*   reddits -> reddits before preprocessing -> df['text']
*   reddit_class -> labels ( bool ) -> df['class']
*   cleanReddits -> cleanText -> df['cleanText']

### Most common ADJ in suicidal/depressive reddits vs most common ADJ in non-suicidal/ depressive reddits

In order to see this, I will need to split my data into suicidal and non suicidal reddits based on their labels.
"""

suicidal = []
non_suicidal = []
# Don't actually think this is needed as the suicidal_verdict will always be 1, and nonsuicidal_verdict will always be 0
suicidal_verdict = []
nonsuicidal_verdict = []

# I am using zip(cleanReddits, reddit_paths) because I don't want to mess up the labels, so I am taking them in pairs
for phrase, verdict  in zip(cleanReddits, reddit_class):
  if int(verdict == "suicide"):
    suicidal.append(phrase)
    suicidal_verdict.append(verdict)
  else:
    non_suicidal.append(phrase)
    nonsuicidal_verdict.append(verdict)

suicidal[1]

"""Most common ADJ in suicidal"""

tokenizer = spacy.load("en_core_web_sm")
suicidal_adj = []
for redd in range((len(suicidal)-1)):
  txt = suicidal[redd]
  token_list = tokenizer(txt)
  for token in token_list:
    if (token.pos_ == 'ADJ'):
      suicidal_adj.append(token)

import os
from collections import Counter
import seaborn as sns

suicidal_adjectives = ' '.join(map(str, suicidal_adj))

d = os.getcwd()
freq = Counter(suicidal_adjectives.split(" "))
sns.set_style("darkgrid")
words = [word[0] for word in freq.most_common(10)]
count = [word[1] for word in freq.most_common(10)]

plt.figure(figsize=(10, 10))

sns_bar = sns.barplot(x=words, y=count)
sns_bar.set_xticklabels(words, rotation=90)
plt.title('Most Used Adjectives in Suicidal')
plt.show()

"""Most Common Used in Non-Suicidal"""

tokenizer = spacy.load("en_core_web_sm")
non_suicidal_adj = []
for redd in range((len(non_suicidal)-1)):
  txt = non_suicidal[redd]
  token_list = tokenizer(txt)
  for token in token_list:
    if (token.pos_ == 'ADJ'):
      non_suicidal_adj.append(token)

import os
from collections import Counter
import seaborn as sns

non_suicidal_adjectives = ' '.join(map(str, non_suicidal_adj))

d = os.getcwd()
freq = Counter(adjectives.split(" "))
sns.set_style("darkgrid")
words = [word[0] for word in freq.most_common(10)]
count = [word[1] for word in freq.most_common(10)]

plt.figure(figsize=(10, 10))

sns_bar = sns.barplot(x=words, y=count)
sns_bar.set_xticklabels(words, rotation=90)
plt.title('Most Used Adjectives in Non Suicidal')
plt.show()

"""### COUNT VECTORIZER

* We are using count vectorizer to implement BoW model

****Count Vectorizer****
Countvectorizer makes it easy for text data to be used directly in machine learning and deep learning models such as text classification. I have 2 text inputs, what happens is that each input is preprocessed, tokenized, and represented as a sparse matrix.



1.   *CountVectorizer* is a text preprocessing class in scikit-learn that is used to convert a collection of text documents into a matrix of token counts.
2.   **vect.fit(reddits)**: The fit() method is called on the CountVectorizer object (vect), and it is passed the variable reddits. This method analyzes the given text data (reddits) and builds an internal vocabulary, which is essentially a dictionary of unique words (or tokens) found in the text corpus. It learns the vocabulary from the provided data.
3. **vect.transform(reddits).toarray()**: This line transforms the given text data (reddits) into a matrix of token counts. The transform() method takes the text data and converts it into a numerical feature representation. The resulting output is a sparse matrix representation of the token counts. The toarray() method is then called on the sparse matrix to convert it into a regular NumPy array.

*   reddit_paths -> indexes for all reddits -> df['index']
*   reddits -> reddits before preprocessing -> df['text']
*   reddit_class -> labels ( bool ) -> df['class']
*   cleanReddits -> cleanText -> df['cleanText']
*   suicidal -> suicidal reddits
*   non_suicidal -> non suicidal reddits

After preproceesing some reddits were nan, so I needed to eliminate them from the list for vectorization - Prove to be of no need after all. This issue was fixed by adding the following line below

vectorized_reddits = df.loc[1:input_size, "cleanText"].astype(str).tolist()
"""

len(cleanReddits)

cleanReddits = df.loc[1:input_size, "cleanText"].astype(str).tolist()

len(cleanReddits)

"""*   reddit_paths -> indexes for all reddits -> df['index']
*   reddits -> reddits before preprocessing -> df['text']
*   reddit_class -> labels ( bool ) -> df['class']
*   cleanReddits -> cleanText -> df['cleanText']
*   cleanLabels -> labels after exclusion of nan reddits
*   suicidal -> suicidal reddits
*   non_suicidal -> non suicidal reddits
"""

vectorized_reddits = df.loc[1:input_size, "cleanText"].astype(str).tolist()

len(vectorized_reddits)

vectorized_reddits

vect =  CountVectorizer()
vect.fit(vectorized_reddits)
vect.transform(vectorized_reddits).toarray()

"""There are a total of 33153 reddit posts proposed for analysis.

The vector vocabulary as 48921 unique words.
"""

len(vect.transform(vectorized_reddits).toarray())

vectorized_reddits

# Unique words
print("Vocabulary size: {}".format(len(vect.vocabulary_)))

# Assigned index of words ( index into array/ list )
print("Vocabulary content:\{}".format(vect.vocabulary_))

sorted_vocabulary = sorted(vect.vocabulary_.items(), key=lambda x: x[0])
sorted_vocabulary[44101]

"""### BoW implementation on data set

First we need to split the data for training and testing as follows:

* 75% for training
* 25% for testing

I am adding this notes from time to time during my projects as I don't want to mess up anything

*   reddit_paths -> indexes for all reddits -> df['index']
*   reddits -> reddits before preprocessing -> df['text']
*   reddit_class -> labels ( bool ) -> df['class']
*   cleanReddits -> cleanText -> df['cleanText']
*   cleanLabels -> labels after exclusion of nan reddits
*   suicidal -> suicidal reddits
*   non_suicidal -> non suicidal reddits

+ nOfRedditsForTest -> number of reddits that are for test purpose
+ trainingReddits

Transform the labels as True or False Values
"""

cleanLabels = []
for i in reddit_class:
  if i == 'suicide':
    cleanLabels.append(1)
  else:
    cleanLabels.append(0)

len(cleanLabels)

"""Checked if all conversion from suicide, non-suicide to 1 and 0 went good"""

f = 1
for i,x in zip(cleanLabels, reddit_class):
    correct = 0
    if i == 1 and x == 'suicide':
      correct = 1
    if i == 0 and x == 'non-suicide':
      correct = 1
    if correct == 0:
      f = 0
print(f)

nOfRedditsForTest = 8289

print("Number of reddits : ", len(cleanReddits))
print("Number of vect_vocabulary: ", len(vect.vocabulary_))
print("Number of tests: ", nOfRedditsForTest)

trainingReddits = cleanReddits[0:(len(cleanReddits)-nOfRedditsForTest)]
testingReddits = cleanReddits[(len(cleanReddits)-nOfRedditsForTest):]

trainingLabels = reddit_class[0:(len(cleanReddits)-nOfRedditsForTest)]
testingLabels = reddit_class[(len(cleanReddits)-nOfRedditsForTest):]


print("Number of reddits for training" , len(trainingReddits))
print("Number of labels for training" , len(trainingLabels))

print("Number of reddits for testing" , len(testingLabels))
print("Number of labels for testing " , len(testingLabels))

len(testingReddits)

# Commented out IPython magic to ensure Python compatibility.
# Run this to ensure TensorFlow 2.x is used
try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass

len(trainingReddits)

print(cleanReddits[5])

print(trainingReddits[5])
print(trainingLabels[5])

print(testingReddits[1])
print(trainingLabels[1])

"""x_train -> reddits for training
x_test -> reddits for testing

y_train -> labels corresponding to train
y_test -> labels corresponding to test
"""

print(type(cleanLabels))

print(type(reddit_class))



# Shuffle the data

from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split

x = vect.transform(cleanReddits).toarray()
y = np.array(cleanLabels)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 2, shuffle = True)

len(x_train)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

print(len(x_train))
print(len(y_train))
print(len(x_test))
print(len(y_test))

"""# svM

"""

from sklearn import svm
from sklearn.metrics import accuracy_score

# C coeficient de regularizare (hiperparam)
model = svm.LinearSVC(C=0.1)
# observati convergence warning
model.fit(x_train, y_train)
tpreds = model.predict(x_test)

print(accuracy_score(y_test, tpreds))

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import svm
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

# Create confusion matrix
cm = confusion_matrix(y_test, tpreds)
# Define labels for the plot
labels = np.unique(y_test)
# Create heatmap
sns.heatmap(cm, annot=True, cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""Calculate F1 score"""

f1 = f1_score(y_test, tpreds)
print("F1 Score:", f1)

"""Some tests:"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import svm
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

# Define a list of regularization coefficients to test
regularization_coefficients = [0.1, 1, 10, 100, 1000]

for C in regularization_coefficients:
    # C coefficient of regularization (hyperparameter)
    model = svm.LinearSVC(C=C)
    # observe convergence warning
    model.fit(x_train, y_train)
    tpreds = model.predict(x_test)

    accuracy = accuracy_score(y_test, tpreds)
    print("Accuracy for C={}: {}".format(C, accuracy))

    # Create confusion matrix
    cm = confusion_matrix(y_test, tpreds)
    # Define labels for the plot
    labels = np.unique(y_test)
    # Create heatmap
    sns.heatmap(cm, annot=True, cmap="Blues", xticklabels=labels, yticklabels=labels)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix for C={}".format(C))
    plt.show()

    # Calculate and print F1 score
    f1 = f1_score(y_test, tpreds, average='weighted')
    print("F1 Score for C={}: {}".format(C, f1))
    print()

"""We tested several coefficients to see which one is a better fit on our SVC for our data.

The smaller the coeficient, the gratest the accuracy.

On our first try we have received the following warning message:  usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn

This suggests that model did not converge with the default number ofinterations, meaning that for an optimal solution we will need to increas the number of iterations. This might help.


"""

print(accuracy_values)

l = [0.01, 0.1, 1, 10, 100, 1000]
coefficients = []
accuracy_scores = []

for coefficient in l:
  model = svm.LinearSVC(C=coefficient)
  model.fit(x_train, y_train)
  tpreds = model.predict(x_test)
  accuracy = accuracy_score(y_test, tpreds)
  accuracy_scores.append(accuracy)
  coefficients.append(str(coefficient))
  print( ' coefficient: ' ,  coefficient ,  'accuracy score: ', accuracy)

import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.metrics import accuracy_score

plt.plot(coefficients, accuracy_scores, marker='o')
plt.xlabel('Coefficient')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Score vs Coefficient SVC')
plt.grid(True)
plt.show()

"""Trying to fix the convergence warning by adding a max_iter param and set it to 10000"""

l = [0.01, 0.1, 1, 10, 100, 1000]
coefficients = []
accuracy_scores = []

for coefficient in l:
  model = svm.LinearSVC(C=coefficient, max_iter=10000)
  model.fit(x_train, y_train)
  tpreds = model.predict(x_test)
  accuracy = accuracy_score(y_test, tpreds)
  accuracy_scores.append(accuracy)
  coefficients.append(str(coefficient))
  print( ' coefficient: ' ,  coefficient ,  'accuracy score: ', accuracy)

"""# SVC"""

from sklearn.metrics import classification_report, confusion_matrix

from sklearn.svm import SVC
model = SVC()

model.fit(x_train, y_train)

predictions = model.predict(x_test)
print(classification_report(y_test, predictions))

"""# Using GridSearchCV

One of the great things about GridSearchCV is that it is a meta-estimator. It takes an estimator like SVC and creates a new estimator, that behaves exactly the same â€“ in this case, like a classifier. You should add refit=True and choose verbose to whatever number you want, the higher the number, the more verbose (verbose just means the text output describing the process).

* https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/

# Gaussian NB

Gaussian NB classifre + train model ( clf )
"""

n_estimators = [10, 1000, 3000, 10.0]
clf = RandomForestClassifier(n_estimators=3000)

clf.fit (x_train,y_train)

y_pred=clf.predict(x_test)

from sklearn import metrics

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""# NN

I am adding this notes from time to time during my projects as I don't want to mess up anything

*   reddit_paths -> indexes for all reddits -> df['index']
*   reddits -> reddits before preprocessing -> df['text']
*   reddit_class -> labels ( bool ) -> df['class']
*   cleanReddits -> cleanText -> df['cleanText']
*   cleanLabels -> labels after exclusion of nan reddits
*   suicidal -> suicidal reddits
*   non_suicidal -> non suicidal reddits

+ nOfRedditsForTest -> number of reddits that are for test purpose
+ trainingReddits

Some imports
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""1.  Using RaggedTensor because it is hard to train a NN to handle sentences of different lengths, so I will use padded sequences.
2.  The desired length will be specified by using maxlen

3. The sentences longer than maxlen will be truncated post or pre-truncation
"""

vocab_size = len(vect.vocabulary_)

vocab_size = int(vocab_size)
vocab_size

# Plot as having good and bad in opposite direction, and i will plot things after. Plot in xy axes and start to determine the good and bad

# Top layer = embedding, where direction of each word will be learned epoch by epoch
# Pull global avarage
embedding_dim = 16
max_length = 100
model = tf.keras.Sequential([
                            # learn epoch by epoch
                             tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),
                             # Pool with a global avarage pooling
                             # adding up the vectors ( as described above )
                             tf.keras.layers.GlobalAveragePooling1D(),
                             tf.keras.layers.Dense(24, activation= 'relu'),
                             tf.keras.layers.Dense(1, activation= 'sigmoid')
])
model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])

# RaggedTensor -when I need to train a nn to handle sentences of different lengths
# For now I will use padded sequences
# I can also specify the desired length using maxlen
# What happens with sentences longer than maxlen: They are truncated post or pre-truncation
oov_tok = "<OOV>"
import torch
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words= vocab_size, oov_token= oov_tok)
# create tokens for every word in the corpus
tokenizer.fit_on_texts(cleanReddits)

word_index = tokenizer.word_index

# turn sentences into sequences of tokens and pad them to the same length
training_sequences = tokenizer.texts_to_sequences(trainingReddits)
training_padded = pad_sequences(training_sequences, maxlen=max_length, padding= 'post', truncating='post')

testing_sequences = tokenizer.texts_to_sequences(testingReddits)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding='post', truncating='post')

import matplotlib.pyplot as plt

num_epochs = 30

history = model.fit(training_padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test), verbose=2)

# Extract training and validation loss from history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']

# Generate x-axis values for epochs
epochs = range(1, num_epochs + 1)

# Plotting the loss
plt.plot(epochs, training_loss, label='Training Loss')
plt.plot(epochs, validation_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt


def plot_graphs(history, string,str2):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.plot(history.history[str2])
  plt.plot(history.history['val_'+str2])
  plt.xlabel("Epochs")
  plt.ylabel(string+" / "+str2)
  plt.legend([string, 'val_'+string, str2, 'val_'+str2])
  plt.savefig('NNAccuracyLoss.pdf')

plot_graphs(history, "accuracy","loss")

"""# BERT MODEL"""

!pip install transformers

from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.metrics import accuracy_score
import tensorflow as tf

"""Load the pre-trained BERT model and tokenizer"""

model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = TFBertForSequenceClassification.from_pretrained(model_name)

"""Tokenize the input data"""

x_train

x_train_tokens = tokenizer.batch_encode_plus(
    trainingReddits,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='tf'
)
x_test_tokens = tokenizer.batch_encode_plus(
    testingReddits,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='tf'
)

"""Convert the tokenized inputs to TensorFlow datasets"""

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(x_train_tokens),
    y_train
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(x_test_tokens),
    y_test
))

"""Prepare the datasets for training and evaluation"""

train_dataset = train_dataset.shuffle(len(x_train)).batch(32)
test_dataset = test_dataset.batch(32)

"""Compile and train the BERT model"""

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
model.fit(train_dataset, epochs=30)

"""Evaluate the model on the test set"""

predictions = model.predict(test_dataset)
y_pred = tf.argmax(predictions.logits, axis=1)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)